dataset:
  dataset: cifar10
  data_dir: /home/ono/Dev/scalingDTP/data
  num_workers: 12
  shuffle: true
  normalize: true
  image_crop_size: 32
  image_crop_pad: 4
  val_split: 0.1
  use_legacy_std: false
model:
  lr_scheduler:
    interval: epoch
    frequency: 1
    T_max: 85
    eta_min: 1.0e-05
  batch_size: 128
  use_scheduler: false
  feedback_training_iterations:
  - 1
  max_epochs: 90
  b_optim:
    type: adam
    lr:
    - 0.0003
    weight_decay: 0.0001
    momentum: 0.9
  noise:
  - 0.4
  - 0.4
  - 0.2
  - 0.2
  - 0.08
  f_optim:
    type: adam
    lr:
    - 0.0003
    weight_decay: 0.0001
    momentum: 0.9
  beta: 0.7
  feedback_samples_per_iteration: 10
  early_stopping_patience: 0
  init_symetric_weights: false
  plot_every: 1000
network:
  activation: elu
  batch_size: 128
  channels:
  - 128
  - 128
  - 256
  - 256
  - 512
  bias: true
trainer:
  _target_: pytorch_lightning.Trainer
  gpus: -1
  strategy: dp
  min_epochs: 1
  max_epochs: 90
  resume_from_checkpoint: null
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/accuracy
    mode: max
    save_top_k: 1
    save_last: true
    verbose: false
    dirpath: checkpoints/
    filename: epoch_{epoch:03d}
    auto_insert_metric_name: false
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/accuracy
    mode: max
    patience: 100
    min_delta: 0
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 1
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
logger: {}
debug: false
verbose: false
seed: null
name: ''
