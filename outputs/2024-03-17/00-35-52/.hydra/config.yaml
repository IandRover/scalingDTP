dataset:
  _target_: pl_bolts.datamodules.cifar10_datamodule.CIFAR10DataModule
  data_dir: /Users/chkao/Documents/GitHub/scalingDTP/data
  val_split: 0.1
  num_workers: 10
  normalize: true
  batch_size: 32
  seed: 42
  shuffle: true
  pin_memory: true
  drop_last: false
  train_transforms:
    _target_: torchvision.transforms.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.transforms.RandomHorizontalFlip
      p: 0.5
    - _target_: torchvision.transforms.transforms.RandomCrop
      size: 32
      padding: 4
      padding_mode: edge
    - _target_: torchvision.transforms.transforms.ToTensor
    - _target_: pl_bolts.transforms.dataset_normalizations.cifar10_normalization
  val_transforms: null
  test_transforms: null
model:
  lr_scheduler:
    interval: epoch
    frequency: 1
    T_max: 85
    eta_min: 1.0e-05
  batch_size: 128
  use_scheduler: true
  max_epochs: 90
  f_optim:
    type: sgd
    lr:
    - 0.05
    weight_decay: null
    momentum: 0.9
  early_stopping_patience: 0
network:
  activation: elu
  channels:
  - 128
  - 128
  - 256
  - 256
  - 512
  bias: true
trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  strategy: dp
  min_epochs: 1
  max_epochs: 90
  resume_from_checkpoint: null
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/accuracy
    mode: max
    save_top_k: 1
    save_last: true
    verbose: false
    dirpath: checkpoints/
    filename: epoch_{epoch:03d}
    auto_insert_metric_name: false
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 1
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
logger: {}
debug: false
verbose: false
seed: null
name: ''
